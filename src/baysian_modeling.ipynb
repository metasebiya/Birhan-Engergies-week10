{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "12hziA-ixyIK1ird9IFPZtp93wBtF0y9H",
      "authorship_tag": "ABX9TyO4UBC/L+AaJsQ+i1nWrC+W",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/metasebiya/Birhan-Engergies-week10/blob/task-2/src/baysian_modeling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pymc3==3.11.5 arviz==0.16.0 --quiet\n",
        "!pip install pandas matplotlib==3.5.3 seaborn==0.11.2 --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kY8PeGz8P0Zz",
        "outputId": "65d684ce-c105-4390-faa6-8a388b33d4e1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.3/10.3 MB\u001b[0m \u001b[31m66.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: Ignored the following yanked versions: 1.11.0, 1.14.0rc1\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: Ignored the following versions that require a different python version: 1.21.2 Requires-Python >=3.7,<3.11; 1.21.3 Requires-Python >=3.7,<3.11; 1.21.4 Requires-Python >=3.7,<3.11; 1.21.5 Requires-Python >=3.7,<3.11; 1.21.6 Requires-Python >=3.7,<3.11; 1.6.2 Requires-Python >=3.7,<3.10; 1.6.3 Requires-Python >=3.7,<3.10; 1.7.0 Requires-Python >=3.7,<3.10; 1.7.1 Requires-Python >=3.7,<3.10; 1.7.2 Requires-Python >=3.7,<3.11; 1.7.3 Requires-Python >=3.7,<3.11; 1.8.0 Requires-Python >=3.8,<3.11; 1.8.0rc1 Requires-Python >=3.8,<3.11; 1.8.0rc2 Requires-Python >=3.8,<3.11; 1.8.0rc3 Requires-Python >=3.8,<3.11; 1.8.0rc4 Requires-Python >=3.8,<3.11; 1.8.1 Requires-Python >=3.8,<3.11\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement scipy<1.8.0,>=1.7.3 (from pymc3) (from versions: 0.8.0, 0.9.0, 0.10.0, 0.10.1, 0.11.0, 0.12.0, 0.12.1, 0.13.0, 0.13.1, 0.13.2, 0.13.3, 0.14.0, 0.14.1, 0.15.0, 0.15.1, 0.16.0, 0.16.1, 0.17.0, 0.17.1, 0.18.0, 0.18.1, 0.19.0, 0.19.1, 1.0.0, 1.0.1, 1.1.0, 1.2.0, 1.2.1, 1.2.2, 1.2.3, 1.3.0, 1.3.1, 1.3.2, 1.3.3, 1.4.0, 1.4.1, 1.5.0, 1.5.1, 1.5.2, 1.5.3, 1.5.4, 1.6.0, 1.6.1, 1.9.0rc1, 1.9.0rc2, 1.9.0rc3, 1.9.0, 1.9.1, 1.9.2, 1.9.3, 1.10.0rc1, 1.10.0rc2, 1.10.0, 1.10.1, 1.11.0rc1, 1.11.0rc2, 1.11.1, 1.11.2, 1.11.3, 1.11.4, 1.12.0rc1, 1.12.0rc2, 1.12.0, 1.13.0rc1, 1.13.0, 1.13.1, 1.14.0rc2, 1.14.0, 1.14.1, 1.15.0rc1, 1.15.0rc2, 1.15.0, 1.15.1, 1.15.2, 1.15.3, 1.16.0rc1, 1.16.0rc2, 1.16.0, 1.16.1)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for scipy<1.8.0,>=1.7.3\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m78.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 1.5.3 which is incompatible.\n",
            "geopandas 1.1.1 requires pandas>=2.0.0, but you have pandas 1.5.3 which is incompatible.\n",
            "dask-cudf-cu12 25.6.0 requires pandas<2.2.4dev0,>=2.0, but you have pandas 1.5.3 which is incompatible.\n",
            "plotnine 0.14.5 requires pandas>=2.2.0, but you have pandas 1.5.3 which is incompatible.\n",
            "xarray 2025.7.1 requires pandas>=2.2, but you have pandas 1.5.3 which is incompatible.\n",
            "cudf-cu12 25.6.0 requires pandas<2.2.4dev0,>=2.0, but you have pandas 1.5.3 which is incompatible.\n",
            "mizani 0.13.5 requires pandas>=2.2.0, but you have pandas 1.5.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.2/35.2 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for matplotlib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "plotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.5.3 which is incompatible.\n",
            "plotnine 0.14.5 requires pandas>=2.2.0, but you have pandas 1.5.3 which is incompatible.\n",
            "bigframes 2.12.0 requires matplotlib>=3.7.1, but you have matplotlib 3.5.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m292.8/292.8 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JSf5uUESQSR2",
        "outputId": "bf60c168-5711-4a45-8e71-0589ea98ccae"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: setuptools==65.5.0 in /usr/local/lib/python3.11/dist-packages (65.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ = 'GNU' # Keep this line\n",
        "os.environ = 'blas.check_openmp=False' # Add this line\n",
        "# Then proceed with your imports and code\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pymc3 as pm\n",
        "import arviz as az\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "Q_5H3uU2cs_8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 1. Data Preparation and EDA ---\n",
        "\n",
        "# Load the Brent oil prices data\n",
        "# Assuming 'BrentOilPrices.csv' is available in the execution environment\n",
        "try:\n",
        "    df = pd.read_csv('drive/Tenx Works/week 10/BrentOilPrices.csv')\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: BrentOilPrices.csv not found. Please ensure the file is in the correct directory.\")\n",
        "    # Create a dummy DataFrame for demonstration if file is not found\n",
        "    data = {\n",
        "        'Date': pd.to_datetime(['1990-01-01', '1990-01-02', '1990-01-03', '2008-01-01', '2008-01-02', '2008-01-03', '2020-01-01', '2020-01-02', '2020-01-03']),\n",
        "        'Price': [20.0, 20.1, 20.2, 100.0, 99.5, 99.0, 50.0, 49.5, 49.0]\n",
        "    }\n",
        "    df = pd.DataFrame(data)\n",
        "    print(\"Using dummy data for demonstration.\")\n",
        "\n",
        "# Convert 'Date' column to datetime objects\n",
        "df = pd.to_datetime(df, format='%d-%b-%y', errors='coerce')\n",
        "\n",
        "# # Drop rows with NaT (Not a Time) if any date conversion failed\n",
        "# df.dropna(subset=, inplace=True)\n",
        "\n",
        "# Sort data by date\n",
        "df = df.sort_values(by='Date').reset_index(drop=True)\n",
        "\n",
        "# Calculate log returns for change point detection, especially for volatility changes\n",
        "# Log returns are often more stationary and better for modeling volatility\n",
        "df = np.log(df['Price'] / df['Price'].shift(1))\n",
        "# # Drop the first row which will have NaN for Log_Return\n",
        "# df.dropna(subset=, inplace=True)\n",
        "\n",
        "# Use the log returns for the model\n",
        "data_to_model = df.values\n",
        "n_data_points = len(data_to_model)\n"
      ],
      "metadata": {
        "id": "IfFa0vmMc5w0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visual inspection of raw prices and log returns\n",
        "plt.figure(figsize=(14, 8))\n",
        "\n",
        "plt.subplot(2, 1, 1)\n",
        "plt.plot(df, df['Price'], label='Brent Oil Price')\n",
        "plt.title('Brent Oil Prices Over Time')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Price ($)')\n",
        "plt.grid(True)\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "yXiBtN_edDF2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.subplot(2, 1, 2)\n",
        "plt.plot(df, data_to_model, label='Daily Log Returns', color='orange')\n",
        "plt.title('Daily Log Returns of Brent Oil Prices')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Log Return')\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bvl9_F3sdEf-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WQ1EIYkTKaAr"
      },
      "outputs": [],
      "source": [
        "# --- 2. Implementing the Bayesian Change Point Model (PyMC3) ---\n",
        "\n",
        "# Define the Bayesian model\n",
        "with pm.Model() as change_point_model:\n",
        "    # The unknown switch point (tau)\n",
        "    # Uniform prior over all possible data points (excluding the very first and last few for stability)\n",
        "    tau = pm.DiscreteUniform('tau', lower=0, upper=n_data_points - 1)\n",
        "\n",
        "    # Parameters for the period before the change point\n",
        "    # Mean of log returns before tau\n",
        "    mu_before = pm.Normal('mu_before', mu=0, sd=0.1)\n",
        "    # Standard deviation of log returns before tau (must be positive)\n",
        "    sigma_before = pm.HalfNormal('sigma_before', sd=0.1)\n",
        "\n",
        "    # Parameters for the period after the change point\n",
        "    # Mean of log returns after tau\n",
        "    mu_after = pm.Normal('mu_after', mu=0, sd=0.1)\n",
        "    # Standard deviation of log returns after tau (must be positive)\n",
        "    sigma_after = pm.HalfNormal('sigma_after', sd=0.1)\n",
        "\n",
        "    # Combine parameters using pm.math.switch based on tau\n",
        "    # This creates a piecewise function for mean and standard deviation\n",
        "    idx = np.arange(n_data_points) # Array of indices for data points\n",
        "    mu = pm.math.switch(idx < tau, mu_before, mu_after)\n",
        "    sigma = pm.math.switch(idx < tau, sigma_before, sigma_after)\n",
        "\n",
        "    # Likelihood function: observed log returns are normally distributed\n",
        "    # with mean and standard deviation determined by the switch function\n",
        "    observation = pm.Normal('observation', mu=mu, sd=sigma, observed=data_to_model)\n",
        "\n",
        "    # Run the MCMC sampler\n",
        "    # tune: number of tuning steps (discarded)\n",
        "    # draws: number of samples to draw from the posterior\n",
        "    # chains: number of independent chains to run\n",
        "    # target_accept: target acceptance ratio for NUTS sampler\n",
        "    trace = pm.sample(draws=2000, tune=1000, chains=2, target_accept=0.9, random_seed=42)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 3. Interpreting the Model Output ---\n",
        "\n",
        "# Check for convergence and summarize results\n",
        "print(\"\\n--- Model Summary ---\")\n",
        "print(az.summary(trace, var_names=['tau', 'mu_before', 'sigma_before', 'mu_after', 'sigma_after']))\n",
        "\n",
        "# Plot trace for visual convergence check\n",
        "print(\"\\n--- Trace Plots (for convergence diagnostics) ---\")\n",
        "az.plot_trace(trace, var_names=['tau', 'mu_before', 'sigma_before', 'mu_after', 'sigma_after'])\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Plot posterior distribution of tau to identify the most probable change point\n",
        "print(\"\\n--- Posterior Distribution of Change Point (tau) ---\")\n",
        "az.plot_posterior(trace, var_names=['tau'], kind='hist', bins=50)\n",
        "plt.title('Posterior Distribution of Change Point (tau)')\n",
        "plt.xlabel('Index of Change Point')\n",
        "plt.ylabel('Probability Density')\n",
        "plt.show()\n",
        "\n",
        "# Get the most probable change point index\n",
        "tau_posterior_mean = int(trace['tau'].mean())\n",
        "tau_posterior_mode = int(trace['tau'].mode()) # Mode is often more indicative for discrete variables\n",
        "\n",
        "print(f\"\\nMost probable change point index (mean): {tau_posterior_mean}\")\n",
        "print(f\"Most probable change point index (mode): {tau_posterior_mode}\")\n",
        "\n",
        "# Convert the index to a date\n",
        "# Adjusting for the log_return shift (first row was dropped)\n",
        "if tau_posterior_mode + 1 < len(df):\n",
        "    change_point_date = df.iloc[tau_posterior_mode + 1]\n",
        "    print(f\"Most probable change point date: {change_point_date.strftime('%Y-%m-%d')}\")\n",
        "else:\n",
        "    print(\"Change point index out of bounds for date conversion.\")\n",
        "\n",
        "# Plot posterior distributions of parameters to quantify impact\n",
        "print(\"\\n--- Posterior Distributions of Mean and Standard Deviation Parameters ---\")\n",
        "az.plot_posterior(trace, var_names=['mu_before', 'mu_after'], kind='hist', bins=30)\n",
        "plt.title('Posterior Distributions of Mean Log Returns')\n",
        "plt.show()\n",
        "\n",
        "az.plot_posterior(trace, var_names=['sigma_before', 'sigma_after'], kind='hist', bins=30)\n",
        "plt.title('Posterior Distributions of Standard Deviation of Log Returns')\n",
        "plt.show()\n",
        "\n",
        "# Quantify the impact (e.g., mean and standard deviation values)\n",
        "mean_before = trace['mu_before'].mean()\n",
        "mean_after = trace['mu_after'].mean()\n",
        "sigma_before = trace['sigma_before'].mean()\n",
        "sigma_after = trace['sigma_after'].mean()\n",
        "\n",
        "print(f\"\\nMean log return before change point: {mean_before:.4f}\")\n",
        "print(f\"Mean log return after change point: {mean_after:.4f}\")\n",
        "print(f\"Standard deviation of log return before change point: {sigma_before:.4f}\")\n",
        "print(f\"Standard deviation of log return after change point: {sigma_after:.4f}\")\n",
        "\n",
        "# Calculate percentage change in standard deviation (as a proxy for volatility change)\n",
        "percent_change_sigma = ((sigma_after - sigma_before) / sigma_before) * 100\n",
        "print(f\"Percentage change in volatility (standard deviation): {percent_change_sigma:.2f}%\")"
      ],
      "metadata": {
        "id": "gX9B9bK3cmEZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 4. Visualizing the Change Point on the Price Series ---\n",
        "plt.figure(figsize=(14, 6))\n",
        "plt.plot(df, df['Price'], label='Brent Oil Price')\n",
        "if 'change_point_date' in locals():\n",
        "    plt.axvline(x=change_point_date, color='r', linestyle='--', label=f'Detected Change Point: {change_point_date.strftime(\"%Y-%m-%d\")}')\n",
        "plt.title('Brent Oil Prices with Detected Change Point')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Price ($)')\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "cLWduswGcmMC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hy4e_6zZcmPa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IyiZG1zCcmTF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FPOBkmeEcmWV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}